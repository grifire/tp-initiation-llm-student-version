{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJ2Kp7pMkqayGf0WewckNg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grifire/tp-initiation-llm-student-version/blob/main/mini_projet_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini projet : Système de Questions-Réponses avec Classement de Qualité\n",
        "\n",
        "**Objectif** : Créer un système qui utilise un LLM pour répondre à des questions sur un domaine spécifique et évaluer la qualité des réponses.\n",
        "\n",
        "**Durée estimée** : 3-4 heures\n",
        "\n",
        "**Description du projet** :\n",
        "1. **Créer un dataset de questions-réponses** : Définir 10-15 questions sur un domaine de votre choix (technologie, cinéma, histoire, etc.) avec leurs réponses de référence\n",
        "2. **Tester différentes approches de prompt** :\n",
        "    - Zero-shot\n",
        "    - One-shot\n",
        "    - Few-shot\n",
        "3. **Comparer les paramètres de génération** : Tester différentes combinaisons de température, top_k, top_p pour voir leur impact\n",
        "4. **Évaluation automatique** : Créer une fonction qui compare la réponse du LLM avec la réponse de référence (vous pouvez utiliser des métriques simples comme la longueur, les mots-clés communs, etc.)\n",
        "5. **Visualisation des résultats** : Afficher un tableau comparatif des performances selon les différentes approches\n",
        "\n",
        "**Livrables attendus** :\n",
        "- Code documenté avec des commentaires\n",
        "- Analyse des résultats en markdown (quelle approche fonctionne le mieux ?)\n",
        "- Au moins 2 visualisations (graphiques ou tableaux)"
      ],
      "metadata": {
        "id": "mHIL5MXXAN3s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYthylJZfm5R"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "# model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "# model_name = \"microsoft/phi-2\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "BC6CRgZRFOK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "dataset est une liste de question/réponse qui serviront de référence pour la réponse du llm\n",
        "\"\"\"\n",
        "dataset = [\n",
        "{\"ask\": \"Quelle est la vitesse de la lumière dans le vide ?\",\n",
        " \"answer\": \"Environ 299 792 kilomètres par seconde.\"},\n",
        "\n",
        "{\"ask\": \"Quel est le nom de notre galaxie ?\",\n",
        " \"answer\": \"La Voie lactée.\"},\n",
        "\n",
        "{\"ask\": \"Quel est l’âge approximatif de l’Univers ?\",\n",
        " \"answer\": \"Environ 13,8 milliards d’années.\"},\n",
        "\n",
        "{\"ask\": \"Qu’est-ce qu’un trou noir ?\",\n",
        " \"answer\": \"Un objet céleste dont la gravité est si forte que rien, pas même la lumière, ne peut s’en échapper.\"},\n",
        "\n",
        "{\"ask\": \"Quelle planète est la plus proche du Soleil ?\",\n",
        " \"answer\": \"Mercure.\"},\n",
        "\n",
        "{\"ask\": \"Quelle planète est connue comme la planète rouge ?\",\n",
        " \"answer\": \"Mars.\"},\n",
        "\n",
        "{\"ask\": \"Qu’est-ce qu’une étoile filante ?\",\n",
        " \"answer\": \"Un météore brûlant en entrant dans l’atmosphère terrestre.\"},\n",
        "\n",
        "{\"ask\": \"Quel est le plus grand planet du système solaire ?\",\n",
        " \"answer\": \"Jupiter.\"},\n",
        "\n",
        "{\"ask\": \"Comment s’appelle le satellite naturel de la Terre ?\",\n",
        " \"answer\": \"La Lune.\"},\n",
        "\n",
        "{\"ask\": \"Qu’est-ce qu’une galaxie ?\",\n",
        " \"answer\": \"Un ensemble gigantesque d’étoiles, de gaz, de poussières et de matière noire.\"},\n",
        "\n",
        "{\"ask\": \"Quelle est la forme approximative de la Voie lactée ?\",\n",
        " \"answer\": \"Une galaxie spirale barrée.\"},\n",
        "\n",
        "{\"ask\": \"Qu’est-ce qu’une supernova ?\",\n",
        " \"answer\": \"L’explosion spectaculaire d’une étoile massive en fin de vie.\"},\n",
        "\n",
        "{\"ask\": \"De quoi est principalement composée l’atmosphère du Soleil ?\",\n",
        " \"answer\": \"Principalement d’hydrogène et d’hélium.\"},\n",
        "\n",
        "{\"ask\": \"Comment appelle-t-on l’ensemble des planètes en orbite autour d’une étoile ?\",\n",
        " \"answer\": \"Un système planétaire.\"},\n",
        "\n",
        "{\"ask\": \"Qu’est-ce que l’ISS ?\",\n",
        " \"answer\": \"La Station spatiale internationale, un laboratoire orbital habité en permanence.\"}\n",
        "]\n"
      ],
      "metadata": {
        "id": "AZrp4oNmBjS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "generate_response prend en entrée un prompt et retourne la réponse générée par le modèle\n",
        "\"\"\"\n",
        "def generate_response(prompt,afficher_prompt = False):\n",
        "  if afficher_prompt :\n",
        "    print(prompt)\n",
        "    print(\"===================================\")\n",
        "  inputs = tokenizer(prompt, return_tensors='pt') # retourner les tenseurs\n",
        "  output = tokenizer.decode(\n",
        "      model.generate(\n",
        "          inputs[\"input_ids\"],\n",
        "          max_new_tokens=50, # max 50 tokens générés\n",
        "      )[0],\n",
        "      skip_special_tokens=True # on ne génère pas les tokens spéciaux <, >, ...\n",
        "  )\n",
        "  return output\n",
        "dialogue=dataset[0]\n",
        "# print(dialogue[\"ask\"])\n",
        "print(generate_response(dialogue[\"ask\"]))"
      ],
      "metadata": {
        "id": "Ag_VA31WIUOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero Shot"
      ],
      "metadata": {
        "id": "zcWEK3ugOlBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ZeroShot génère un prompte simple dans le but d'aider le LLM à répondre à la question\n",
        "\"\"\"\n",
        "def ZeroShot(question):\n",
        "  prompt = f\"Répond à la question suivante : \\n {question} \\n réponse : \"\n",
        "  return prompt\n",
        "\n",
        "dialogue=dataset[2]\n",
        "print(generate_response(ZeroShot(dialogue[\"ask\"]), True))\n",
        "#"
      ],
      "metadata": {
        "id": "2rHROCY3OUJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Shot"
      ],
      "metadata": {
        "id": "iVZoVbGC6nmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "OneShot Génère un exemple de question réponse pour faire au LLM une démonstration\n",
        "de comment générer une réponse.\n",
        "\"\"\"\n",
        "def OneShot(question, ds, index,afficher_prompt = False):\n",
        "  ref = ds[index]\n",
        "  prompt = f\"\"\"\n",
        "Question :\n",
        "\n",
        "{ref['ask']}\n",
        "\n",
        "\n",
        "Réponse :\n",
        "{ref['answer']}\n",
        "\n",
        "\n",
        "Question :\n",
        "\n",
        "{question}\n",
        "\n",
        "Réponse :\"\"\"\n",
        "\n",
        "  return generate_response(prompt, afficher_prompt)\n",
        "\n",
        "dialogue=dataset[2]\n",
        "print(OneShot(dialogue[\"ask\"], dataset, 0, True))\n",
        "#"
      ],
      "metadata": {
        "id": "z_ByNlIhQzY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi Shot"
      ],
      "metadata": {
        "id": "6QYZrxlG6thF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "make_prompt est la version multi shot du OneShot.\n",
        "il prend en entrée une question, un dataset, et une liste d'index du dataset\n",
        "\"\"\"\n",
        "def make_prompt(question, ds, indexs = [0]) :\n",
        "  prompt = \"\";\n",
        "  for i in indexs :\n",
        "    ref = ds[i]\n",
        "    prompt += f\"\"\"\n",
        "Question :\n",
        "\n",
        "{ref['ask']}\n",
        "\n",
        "Réponse :\n",
        "{ref['answer']}\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  prompt += f\"\"\"\n",
        "Question :\n",
        "\n",
        "{question}\n",
        "\n",
        "Réponse :\n",
        "\"\"\"\n",
        "  return prompt\n",
        "\n",
        "question = \"Quelle est la superficie de la Terre?\"\n",
        "print(generate_response(make_prompt(question, dataset,[0,1,2,3,4,5,6]), True))"
      ],
      "metadata": {
        "id": "ChOjCcIMUxAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "evaluation permet d'évalué un prompte avec la réponse du LLM et la réponse attendu.\n",
        "Il génère un tabeau de 3 valeurs : le nombre de mot de la réponse, le nombre de mot attendu,\n",
        "et le nombre en commun entre la réponse du LLM et celle du LLM\n",
        "\"\"\"\n",
        "def evaluation(prompt,reponse,attendu) :\n",
        "\n",
        "  print(f\"Prompte : {prompt}\")\n",
        "  print(f\"Réponse : {reponse.replace(prompt,\"\")}\")\n",
        "  print(f\"Réponse attendue : {attendu}\")\n",
        "  nb_mots = len(reponse.split())\n",
        "  nb_mots_ref = len(attendu.split())\n",
        "  nb_mots_communs = len(set(reponse.split()).intersection(set(attendu.split())))\n",
        "  tableau = f\"\"\"\n",
        "  Nombre de mots obtenu \\tNombre de mots réponse attendue \\tNombre de mots en commun\n",
        "  {nb_mots}\\t\\t\\t\\t{nb_mots_ref}\\t\\t\\t\\t\\t{nb_mots_communs}\n",
        "===============================================================================================================\n",
        "  \"\"\"\n",
        "  print(tableau)"
      ],
      "metadata": {
        "id": "0y0Pv-5W_HZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tableau de comparaison\")\n",
        "print(\"Zero Shot\")\n",
        "evaluation(ZeroShot(dataset[2][\"ask\"]), generate_response(ZeroShot(dataset[2][\"ask\"])), dataset[2][\"answer\"])\n",
        "print(\"One Shot\")\n",
        "evaluation(make_prompt(question, dataset,[0]), generate_response(make_prompt(question, dataset,[0])), dataset[2][\"answer\"])\n",
        "print(\"Multi Shot (x2)\")\n",
        "evaluation(make_prompt(question, dataset,[0,1]), generate_response(make_prompt(question, dataset,[0,1])), dataset[2][\"answer\"])\n",
        "print(\"Multi Shot (x7)\")\n",
        "evaluation(make_prompt(question, dataset,[0,1,2,3,4,5,6]), generate_response(make_prompt(question, dataset,[0,1,2,3,4,5,6])), dataset[2][\"answer\"])"
      ],
      "metadata": {
        "id": "2LVANzdzB9NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion Partie 2 Zero/One/Multi shot\n",
        "\n",
        "Le Zero shot semble être la solution la plus adapté car le llm à tendance a vouloir poursuivre le schéma et génère de nouveau de question/réponse après avoir répondu.\n",
        "Avec un prompte Zero shot il répond simplement à la question"
      ],
      "metadata": {
        "id": "dEzmjA3J60e1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response_plus(temperature, top_k, top_p, sampling, prompt,attendu):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            do_sample=sampling,\n",
        "\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    print(f\"T={temperature}, top_k={top_k}, top_p={top_p}, sampling={sampling}\\n\") #.replace(prompt,\"\")\n",
        "    evaluation(prompt,output,attendu)\n",
        "    print()\n",
        "\n",
        "\n",
        "prompt = dataset[2][\"ask\"]\n",
        "prompt = ZeroShot(prompt)\n",
        "generate_response_plus(1, 50, 1, True, prompt, dataset[2][\"answer\"])\n",
        "generate_response_plus(1, 50, 1, False, prompt, dataset[2][\"answer\"])\n",
        "generate_response_plus(50, 50, 1, True, prompt, dataset[2][\"answer\"])\n",
        "generate_response_plus(1, 50, 0.1, True, prompt, dataset[2][\"answer\"])\n",
        "generate_response_plus(200, 50, 0.1, True, prompt, dataset[2][\"answer\"])\n",
        "generate_response_plus(50, 3, 0.7, True, prompt, dataset[2][\"answer\"])\n",
        "generate_response_plus(10, 30, 5, True, prompt, dataset[2][\"answer\"])\n",
        "generate_response_plus(10, 30, 5, False, prompt, dataset[2][\"answer\"])\n",
        "generate_response_plus(200, 600, 1, True, prompt, dataset[2][\"answer\"])\n",
        "generate_response_plus(200, 600, 0.99, True, prompt, dataset[2][\"answer\"])\n",
        "generate_response_plus(200, 600, 0.99, False, prompt, dataset[2][\"answer\"])\n",
        "\n",
        "print(f\"\\n\\nRéponse attendu :\\n{dataset[2][\"answer\"]}\")"
      ],
      "metadata": {
        "id": "OnH1Rz7vb4eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_response_plus(1, 50, 0.1, True, prompt, dataset[2][\"answer\"])\n",
        "generate_response_plus(1, 50, 0.2, True, prompt, dataset[2][\"answer\"])\n",
        "generate_response_plus(1, 50, 0.5, True, prompt, dataset[2][\"answer\"])\n",
        "generate_response_plus(1, 50, 0.7, True, prompt, dataset[2][\"answer\"])\n",
        "generate_response_plus(1, 50, 0.8, True, prompt, dataset[2][\"answer\"])\n",
        "generate_response_plus(1, 50, 0.99, True, prompt, dataset[2][\"answer\"])\n",
        "generate_response_plus(1, 50, 1, True, prompt, dataset[2][\"answer\"])"
      ],
      "metadata": {
        "id": "_LPp0svEMI8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Les paramêtre :\n",
        "Sampling : Autorise le LLM à être inventif (Vrai s'il doit être inventif). Les paramêtres suivants ne sont pas impactant si il est réglé sur Faux.\n",
        "\n",
        "Temperature : Le degrés d'inventivité. Plus il est élevé, plus la réponses obtenu risque d'être hors sujet ou chaotique.\n",
        "\n",
        "top_k : le nombre de token qu'il peut prendre en considération.\n",
        "\n",
        "top_p : Si il est plus petit que 1, il permet d'influencé sur la réponse. Plus il est proche de 1 plus la réponse obtenu risque de fluctuer. Contrairement à la température, il n'influe pas sur la logique de la réponse même si celle-si est fausse.\n"
      ],
      "metadata": {
        "id": "RGtNAnA4FlGl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CKoBb5QqPyVf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}